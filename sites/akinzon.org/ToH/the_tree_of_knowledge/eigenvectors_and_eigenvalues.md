# Eigenvectors and Eigenvalues

Up: [Linear Algebra](linear_algebra)
Brother(s):
TARGET DECK

In the general case of [Matrices|matrix](matrices|matrix) transformations, they transform one vector into another, and usually they point in a different direction to before.

But some special vectors remain on their span, still pointing in the same direction!

These are known as $Eigenvectors.$

Applying a matrix transformation to eigenvectors is the same as scaling it by a number!

So we say: $$ Av=λv $$
Where $λ$ is a constant.

This is the same as:
$$(A−λ I) v=0$$
Where $\lambda I$ is the identity matrix scaled by $\lambda$. 

Solutions only exist if the column vectors of the right hand side are linearly dependent or:
$$ det(A−λ I)=0 $$

For example:
	![Pasted image 20231208230321.png](pasted_image_20231208230321.png)


#### Uses of the Eigen-basis
We know that:
	![Pasted image 20231208233826.png](pasted_image_20231208233826.png)
	![Pasted image 20240108210356.png](pasted_image_20240108210356.png)

So, from that we can derive that:
	![Pasted image 20231208233852.png](pasted_image_20231208233852.png)


Since we know what $M$ ($n$ eigenvectors) is, and the eigen-basis (each eigenvalue along the diagonal), all we need to know is the inverse of $M$ and we can do crazy things like:
	![Pasted image 20231208233937.png](pasted_image_20231208233937.png)


#### Diagonalisation
Matrices are considered diagonalisable if there is an invertible matrix such that $M^{-1}AM$ is a diagonal matrix.

Matrix that can be transformed into each other using this formula are called similar to each other.

Diagonal matrices have columns where the only non-zero value in each column is an $eigenvector$.

**Important to note:** If A is an n x n matrix and the sum of the dimensions of an eigenspace (the space generated by all eigenvectors with the same eigenvalue) is n, then A is diagonalisable.




















#### Why:
#### How:









